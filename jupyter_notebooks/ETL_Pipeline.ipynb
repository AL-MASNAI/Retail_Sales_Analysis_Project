{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd0f93c",
   "metadata": {},
   "source": [
    "# 📉 Retail Sales Data Analysis: ETL Pipeline\n",
    "\n",
    "## 🎯 Project Goal\n",
    "\n",
    "This notebook details the **Extract, Transform, Load (ETL)** pipeline for the retail sales data. The primary objective is to clean and prepare the raw data from three CSV files into a single, cohesive dataset ready for analysis. The pipeline is designed to be automated and includes documentation and validation checks to ensure data integrity.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1 Extract: Loading Data from CSV\n",
    "\n",
    "This initial step involves loading the raw data from the provided CSV files into pandas DataFrames. This is the \"extraction\" phase of the ETL process. The code below loads the data and provides a quick inspection of the first few rows and data types to understand the initial state of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36294d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Data extracted successfully!\n",
      "📦 Stores data shape:   (45, 3)\n",
      "📊 Features data shape: (8190, 12)\n",
      "🛒 Sales data shape:    (421570, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>202307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B</td>\n",
       "      <td>37392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>205863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B</td>\n",
       "      <td>34875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store Type    Size\n",
       "0      1    A  151315\n",
       "1      2    A  202307\n",
       "2      3    B   37392\n",
       "3      4    A  205863\n",
       "4      5    B   34875"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Date</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>MarkDown1</th>\n",
       "      <th>MarkDown2</th>\n",
       "      <th>MarkDown3</th>\n",
       "      <th>MarkDown4</th>\n",
       "      <th>MarkDown5</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>IsHoliday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>05/02/2010</td>\n",
       "      <td>42.31</td>\n",
       "      <td>2.572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.096358</td>\n",
       "      <td>8.106</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12/02/2010</td>\n",
       "      <td>38.51</td>\n",
       "      <td>2.548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.242170</td>\n",
       "      <td>8.106</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19/02/2010</td>\n",
       "      <td>39.93</td>\n",
       "      <td>2.514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.289143</td>\n",
       "      <td>8.106</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>26/02/2010</td>\n",
       "      <td>46.63</td>\n",
       "      <td>2.561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.319643</td>\n",
       "      <td>8.106</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>05/03/2010</td>\n",
       "      <td>46.50</td>\n",
       "      <td>2.625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.350143</td>\n",
       "      <td>8.106</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store        Date  Temperature  Fuel_Price  MarkDown1  MarkDown2  \\\n",
       "0      1  05/02/2010        42.31       2.572        NaN        NaN   \n",
       "1      1  12/02/2010        38.51       2.548        NaN        NaN   \n",
       "2      1  19/02/2010        39.93       2.514        NaN        NaN   \n",
       "3      1  26/02/2010        46.63       2.561        NaN        NaN   \n",
       "4      1  05/03/2010        46.50       2.625        NaN        NaN   \n",
       "\n",
       "   MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment  IsHoliday  \n",
       "0        NaN        NaN        NaN  211.096358         8.106      False  \n",
       "1        NaN        NaN        NaN  211.242170         8.106       True  \n",
       "2        NaN        NaN        NaN  211.289143         8.106      False  \n",
       "3        NaN        NaN        NaN  211.319643         8.106      False  \n",
       "4        NaN        NaN        NaN  211.350143         8.106      False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>IsHoliday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>05/02/2010</td>\n",
       "      <td>24924.50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12/02/2010</td>\n",
       "      <td>46039.49</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19/02/2010</td>\n",
       "      <td>41595.55</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26/02/2010</td>\n",
       "      <td>19403.54</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>05/03/2010</td>\n",
       "      <td>21827.90</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  Dept        Date  Weekly_Sales  IsHoliday\n",
       "0      1     1  05/02/2010      24924.50      False\n",
       "1      1     1  12/02/2010      46039.49       True\n",
       "2      1     1  19/02/2010      41595.55      False\n",
       "3      1     1  26/02/2010      19403.54      False\n",
       "4      1     1  05/03/2010      21827.90      False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def extract_raw_data(raw_data_dir='../data/raw_data/', preview=True):\n",
    "    \"\"\"\n",
    "    Extracts raw retail sales data from CSV files.\n",
    "\n",
    "    Parameters:\n",
    "        raw_data_dir (str): Path to the directory containing raw CSV files.\n",
    "        preview (bool): Whether to display head and shape of each DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        stores_df, features_df, sales_df: Loaded pandas DataFrames.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        stores_file = os.path.join(raw_data_dir, 'Stores.csv')\n",
    "        features_file = os.path.join(raw_data_dir, 'Features.csv')\n",
    "        sales_file = os.path.join(raw_data_dir, 'Sales.csv')\n",
    "\n",
    "        stores_df = pd.read_csv(stores_file)\n",
    "        features_df = pd.read_csv(features_file)\n",
    "        sales_df = pd.read_csv(sales_file)\n",
    "\n",
    "        if preview:\n",
    "            print(\"\\n✅ Data extracted successfully!\")\n",
    "            print(f\"📦 Stores data shape:   {stores_df.shape}\")\n",
    "            print(f\"📊 Features data shape: {features_df.shape}\")\n",
    "            print(f\"🛒 Sales data shape:    {sales_df.shape}\")\n",
    "            display(stores_df.head())\n",
    "            display(features_df.head())\n",
    "            display(sales_df.head())\n",
    "\n",
    "        return stores_df, features_df, sales_df\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n❌ File not found: {e.filename}\")\n",
    "        print(\"🔍 Please ensure the CSV files are located in the '../data/raw_data/' directory.\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "# Run extraction\n",
    "stores_df, features_df, sales_df = extract_raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a2413b",
   "metadata": {},
   "source": [
    "### 1.2 Transform: Cleaning and Preprocessing Data\n",
    "\n",
    "This is the core of the ETL pipeline where data is cleaned, transformed, and merged. The transformation logic is as follows:\n",
    "\n",
    "- **Data Type Conversion**: The `Date` column in the `features_df` and `sales_df` is converted from a string to a datetime object for proper time-series analysis.\n",
    "- **Data Merging**: The three DataFrames are merged into a single, comprehensive DataFrame (`merged_df`) using `Store` and `Date` as common keys.\n",
    "- **Handling Missing Values**:\n",
    "  - `MarkDown` columns: Missing values are filled with `0.0`, as `NaN` likely indicates that no markdown was applied.\n",
    "  - `CPI` and `Unemployment`: Missing values are imputed using the mean of their respective columns.\n",
    "- **Feature Engineering**: A new feature, `Sales_Holiday_Difference`, is created to quantify the difference between a week's sales and the average sales for a holiday or non-holiday period.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e17ff7",
   "metadata": {},
   "source": [
    "Step 1: Convert Date Columns\n",
    "\n",
    "- Convert Date columns in features_df and sales_df to datetime format.\n",
    "- Use dayfirst=True to correctly interpret DD/MM/YYYY.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07545000",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, name in [(features_df, 'features_df'), (sales_df, 'sales_df')]:\n",
    "    if 'Date' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)\n",
    "    else:\n",
    "        print(f\"⚠️ 'Date' column missing in {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7432ed61",
   "metadata": {},
   "source": [
    "Step 2: Merge DataFrames\n",
    "\n",
    "- Merge sales_df with features_df on Store, Date, and IsHoliday.\n",
    "- Then merge the result with stores_df on Store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9f8599b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Data merged successfully. Final shape:\n",
      "🧮 (421570, 16)\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.merge(sales_df, features_df, on=[\n",
    "                     'Store', 'Date', 'IsHoliday'], how='left')\n",
    "merged_df = pd.merge(merged_df, stores_df, on='Store', how='left')\n",
    "\n",
    "print(\"\\n✅ Data merged successfully. Final shape:\")\n",
    "print(f\"🧮 {merged_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd52d319",
   "metadata": {},
   "source": [
    "Step 3: Handle Missing Values\n",
    "\n",
    "- Fill missing MarkDown values with 0.0 (assumed no promotion).\n",
    "- Impute missing CPI and Unemployment using forward-fill and backward-fill\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a571310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧼 Missing values imputed.\n",
      "Store           0\n",
      "Dept            0\n",
      "Date            0\n",
      "Weekly_Sales    0\n",
      "IsHoliday       0\n",
      "Temperature     0\n",
      "Fuel_Price      0\n",
      "MarkDown1       0\n",
      "MarkDown2       0\n",
      "MarkDown3       0\n",
      "MarkDown4       0\n",
      "MarkDown5       0\n",
      "CPI             0\n",
      "Unemployment    0\n",
      "Type            0\n",
      "Size            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "markdown_cols = ['MarkDown1', 'MarkDown2',\n",
    "                 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
    "for col in markdown_cols:\n",
    "    if col in merged_df.columns:\n",
    "        merged_df[col] = merged_df[col].fillna(0)\n",
    "\n",
    "for col in ['CPI', 'Unemployment']:\n",
    "    if col in merged_df.columns:\n",
    "        merged_df[col] = merged_df[col].ffill().bfill()\n",
    "\n",
    "print(\"\\n🧼 Missing values imputed.\")\n",
    "print(merged_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94469bcf",
   "metadata": {},
   "source": [
    "Step 4: Feature Engineering\n",
    "\n",
    "- Create Sales_Holiday_Difference: deviation from average sales for holiday vs. non-holiday weeks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35b5542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Sales_Holiday_Difference'] = merged_df.groupby('IsHoliday')['Weekly_Sales'].transform(\n",
    "    lambda x: x - x.mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9c7165",
   "metadata": {},
   "source": [
    "Step 5: Inflation Adjustment & Base Sales\n",
    "\n",
    "- Adjust Weekly_Sales for inflation using the first CPI value as baseline.\n",
    "- Identify promotional periods and calculate average non-promotional sales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66a595e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Additional Metrics Calculated:\n",
      "🧮 Baseline CPI: 211.0963582\n",
      "📊 Avg Weekly Sales (Non-Promotional): 15871.52\n"
     ]
    }
   ],
   "source": [
    "baseline_cpi = merged_df['CPI'].iloc[0]\n",
    "merged_df['Inflation_Adjusted_Weekly_Sales'] = merged_df['Weekly_Sales'] / \\\n",
    "    (merged_df['CPI'] / baseline_cpi)\n",
    "\n",
    "merged_df['Is_Promotional_Period'] = (merged_df[markdown_cols] > 0).any(axis=1)\n",
    "base_sales = merged_df.loc[~merged_df['Is_Promotional_Period'],\n",
    "                           'Weekly_Sales'].mean()\n",
    "\n",
    "print(\"\\n📈 Additional Metrics Calculated:\")\n",
    "print(f\"🧮 Baseline CPI: {baseline_cpi}\")\n",
    "print(f\"📊 Avg Weekly Sales (Non-Promotional): {base_sales:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c483e67",
   "metadata": {},
   "source": [
    "Step 6: Final Sorting and Preview\n",
    "\n",
    "- Sort by Store, Dept, and Date, then reset index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c0f241b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final data preparation complete. Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>MarkDown1</th>\n",
       "      <th>MarkDown2</th>\n",
       "      <th>MarkDown3</th>\n",
       "      <th>MarkDown4</th>\n",
       "      <th>MarkDown5</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size</th>\n",
       "      <th>Sales_Holiday_Difference</th>\n",
       "      <th>Inflation_Adjusted_Weekly_Sales</th>\n",
       "      <th>Is_Promotional_Period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>24924.50</td>\n",
       "      <td>False</td>\n",
       "      <td>42.31</td>\n",
       "      <td>2.572</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>211.096358</td>\n",
       "      <td>8.106</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>9023.054931</td>\n",
       "      <td>24924.500000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>46039.49</td>\n",
       "      <td>True</td>\n",
       "      <td>38.51</td>\n",
       "      <td>2.548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>211.242170</td>\n",
       "      <td>8.106</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>29003.666813</td>\n",
       "      <td>46007.710873</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>41595.55</td>\n",
       "      <td>False</td>\n",
       "      <td>39.93</td>\n",
       "      <td>2.514</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>211.289143</td>\n",
       "      <td>8.106</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>25694.104931</td>\n",
       "      <td>41557.597337</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-26</td>\n",
       "      <td>19403.54</td>\n",
       "      <td>False</td>\n",
       "      <td>46.63</td>\n",
       "      <td>2.561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>211.319643</td>\n",
       "      <td>8.106</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>3502.094931</td>\n",
       "      <td>19383.037819</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-03-05</td>\n",
       "      <td>21827.90</td>\n",
       "      <td>False</td>\n",
       "      <td>46.50</td>\n",
       "      <td>2.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>211.350143</td>\n",
       "      <td>8.106</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>5926.454931</td>\n",
       "      <td>21801.689528</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  Dept       Date  Weekly_Sales  IsHoliday  Temperature  Fuel_Price  \\\n",
       "0      1     1 2010-02-05      24924.50      False        42.31       2.572   \n",
       "1      1     1 2010-02-12      46039.49       True        38.51       2.548   \n",
       "2      1     1 2010-02-19      41595.55      False        39.93       2.514   \n",
       "3      1     1 2010-02-26      19403.54      False        46.63       2.561   \n",
       "4      1     1 2010-03-05      21827.90      False        46.50       2.625   \n",
       "\n",
       "   MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  \\\n",
       "0        0.0        0.0        0.0        0.0        0.0  211.096358   \n",
       "1        0.0        0.0        0.0        0.0        0.0  211.242170   \n",
       "2        0.0        0.0        0.0        0.0        0.0  211.289143   \n",
       "3        0.0        0.0        0.0        0.0        0.0  211.319643   \n",
       "4        0.0        0.0        0.0        0.0        0.0  211.350143   \n",
       "\n",
       "   Unemployment Type    Size  Sales_Holiday_Difference  \\\n",
       "0         8.106    A  151315               9023.054931   \n",
       "1         8.106    A  151315              29003.666813   \n",
       "2         8.106    A  151315              25694.104931   \n",
       "3         8.106    A  151315               3502.094931   \n",
       "4         8.106    A  151315               5926.454931   \n",
       "\n",
       "   Inflation_Adjusted_Weekly_Sales  Is_Promotional_Period  \n",
       "0                     24924.500000                  False  \n",
       "1                     46007.710873                  False  \n",
       "2                     41557.597337                  False  \n",
       "3                     19383.037819                  False  \n",
       "4                     21801.689528                  False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_df = merged_df.sort_values(\n",
    "    by=['Store', 'Dept', 'Date']).reset_index(drop=True)\n",
    "print(\"\\n✅ Final data preparation complete. Preview:\")\n",
    "display(merged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a696eb8",
   "metadata": {},
   "source": [
    "### 1.5 Validate: Data Quality Checks\n",
    "\n",
    "This step ensures the integrity and quality of the transformed data. We will validate that:\n",
    "\n",
    "1.  There are no missing values after the imputation process.\n",
    "2.  The new `Sales_Holiday_Difference` column has been created successfully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c35a1c0",
   "metadata": {},
   "source": [
    "Step 1: Check for Missing Values\n",
    "\n",
    "- Ensure all missing values have been handled.\n",
    "- This includes MarkDown, CPI, and Unemployment columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2310458c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Checking for null values after transformation:\n",
      "Store                              0\n",
      "Dept                               0\n",
      "Date                               0\n",
      "Weekly_Sales                       0\n",
      "IsHoliday                          0\n",
      "Temperature                        0\n",
      "Fuel_Price                         0\n",
      "MarkDown1                          0\n",
      "MarkDown2                          0\n",
      "MarkDown3                          0\n",
      "MarkDown4                          0\n",
      "MarkDown5                          0\n",
      "CPI                                0\n",
      "Unemployment                       0\n",
      "Type                               0\n",
      "Size                               0\n",
      "Sales_Holiday_Difference           0\n",
      "Inflation_Adjusted_Weekly_Sales    0\n",
      "Is_Promotional_Period              0\n",
      "dtype: int64\n",
      "\n",
      "✅ No missing values detected.\n"
     ]
    }
   ],
   "source": [
    "print(\"🔎 Checking for null values after transformation:\")\n",
    "null_counts = merged_df.isnull().sum()\n",
    "print(null_counts)\n",
    "\n",
    "if null_counts.sum() == 0:\n",
    "    print(\"\\n✅ No missing values detected.\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Warning: Missing values still present. Please review the imputation steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca15f6aa",
   "metadata": {},
   "source": [
    "Step 2: Validate Feature Creation\n",
    "\n",
    "- Confirm that Sales_Holiday_Difference exists and contains valid values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e7a989b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 'Sales_Holiday_Difference' column created successfully.\n",
      "📊 First 5 values:\n",
      "0     9023.054931\n",
      "1    29003.666813\n",
      "2    25694.104931\n",
      "3     3502.094931\n",
      "4     5926.454931\n",
      "Name: Sales_Holiday_Difference, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "if 'Sales_Holiday_Difference' in merged_df.columns:\n",
    "    print(\"\\n✅ 'Sales_Holiday_Difference' column created successfully.\")\n",
    "    print(\"📊 First 5 values:\")\n",
    "    print(merged_df['Sales_Holiday_Difference'].head())\n",
    "else:\n",
    "    print(\"\\n❌ Error: 'Sales_Holiday_Difference' column was not created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f17afa5",
   "metadata": {},
   "source": [
    "Additional Checks (for deeper validation):\n",
    "\n",
    "- Confirm expected data types for key columns (Date, Weekly_Sales, CPI)\n",
    "- Check for duplicate rows\n",
    "- Validate value ranges (e.g. no negative sales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e7ef573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 Data Validation Function\n",
    "def validate_data(df):\n",
    "    \"\"\"\n",
    "    Validates key aspects of the transformed dataset.\n",
    "    Prints warnings or confirmations for:\n",
    "    - Expected data types\n",
    "    - Duplicate rows\n",
    "    - Value ranges (e.g. no negative sales)\n",
    "    \"\"\"\n",
    "    print(\"🔍 Starting data validation...\\n\")\n",
    "\n",
    "    # 1. Check expected data types\n",
    "    expected_types = {\n",
    "        'Date': 'datetime64[ns]',\n",
    "        'Weekly_Sales': 'float',\n",
    "        'CPI': 'float'\n",
    "    }\n",
    "\n",
    "    print(\"📌 Checking column data types:\")\n",
    "    for col, expected_type in expected_types.items():\n",
    "        if col in df.columns:\n",
    "            actual_type = df[col].dtype\n",
    "            if str(actual_type) == expected_type:\n",
    "                print(f\"✅ {col}: {actual_type} (as expected)\")\n",
    "            else:\n",
    "                print(f\"⚠️ {col}: {actual_type} (expected {expected_type})\")\n",
    "        else:\n",
    "            print(f\"❌ {col} column not found in DataFrame.\")\n",
    "\n",
    "    # 2. Check for duplicate rows\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    if duplicate_count == 0:\n",
    "        print(\"\\n✅ No duplicate rows found.\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ {duplicate_count} duplicate rows detected.\")\n",
    "\n",
    "    # 3. Validate value ranges\n",
    "    print(\"\\n📊 Checking for negative values in 'Weekly_Sales':\")\n",
    "    if 'Weekly_Sales' in df.columns:\n",
    "        negative_sales = (df['Weekly_Sales'] < 0).sum()\n",
    "        if negative_sales == 0:\n",
    "            print(\"✅ No negative sales values.\")\n",
    "        else:\n",
    "            print(f\"⚠️ {negative_sales} rows have negative sales values.\")\n",
    "    else:\n",
    "        print(\"❌ 'Weekly_Sales' column not found.\")\n",
    "\n",
    "    print(\"\\n✅ Data validation complete.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e355d2e1",
   "metadata": {},
   "source": [
    "### 1.3 Load: Storing the Transformed Data\n",
    "\n",
    "The final step of the ETL process involves saving the cleaned and transformed DataFrame to a new CSV file. This file, `cleaned_sales_data.csv`, is the final output of the pipeline and is ready for in-depth analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b37e8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of column index: <class 'pandas.core.indexes.base.Index'>\n",
      "Column index contents: ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size', 'Sales_Holiday_Difference', 'Inflation_Adjusted_Weekly_Sales', 'Is_Promotional_Period']\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of column index:\", type(merged_df.columns))\n",
    "print(\"Column index contents:\", merged_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd16313b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Data has been successfully saved to 'c:\\Users\\masna\\OneDrive\\Documents\\vscode-projects\\Project Assessment Folder\\Retail_Sales_Analysis_Project\\Retail_Sales_Analysis_Project\\jupyter_notebooks\\data\\processed_data\\cleaned_sales_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 🧼 Sanitize index and column headers\n",
    "merged_df.index = pd.RangeIndex(start=0, stop=len(merged_df), step=1)\n",
    "merged_df.columns = merged_df.columns.astype(str)\n",
    "\n",
    "# 📁 Define output path using os.getcwd() for interactive environments\n",
    "project_root = os.getcwd()\n",
    "output_dir = os.path.join(project_root, 'data', 'processed_data')\n",
    "output_file = os.path.join(output_dir, 'cleaned_sales_data.csv')\n",
    "\n",
    "# 📂 Create directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 💾 Save to CSV\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n✅ Data has been successfully saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aae3c17",
   "metadata": {},
   "source": [
    "### 1.4 Automate & 1.6 Document: Final Notes\n",
    "\n",
    "This entire notebook serves as the automation and documentation for the ETL process.\n",
    "\n",
    "- **Automation**: By running all cells in this notebook sequentially, the full ETL pipeline is executed automatically, from data extraction to loading the final cleaned dataset. This script can be scheduled or integrated into a larger workflow.\n",
    "\n",
    "- **Documentation**: The markdown cells provide clear explanations of the data sources, the transformation logic, and the validation checks performed. This documentation makes the process transparent and easy for others to understand and replicate.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
